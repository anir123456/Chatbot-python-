{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14348,"status":"ok","timestamp":1749029016161,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"wfPTWZWea_H2","outputId":"4917ed8f-8d4a-4981-bc1c-cf9b531d30dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tflearn in /usr/local/lib/python3.11/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.26.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.17.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tflearn) (11.2.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["!pip install tflearn\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.stem.lancaster import LancasterStemmer\n","stemmer = LancasterStemmer()\n","\n","# Libraries needed for Tensorflow processing\n","import tensorflow as tf\n","import numpy as np\n"," # This should import successfully after installing with pip\n","import random\n","import json"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7XUV1wW4bejn","executionInfo":{"status":"ok","timestamp":1749029016187,"user_tz":-330,"elapsed":15,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"outputs":[],"source":["import json\n","with open('intents.json') as f:\n","   data = json.load(f)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1749029016223,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"cjh_glHwdPyE","outputId":"bce03358-8291-4a89-c283-7e67e4d6625e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'intents': [{'tag': 'greeting',\n","   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n","   'responses': ['Hello, thanks for visiting',\n","    'Good to see you again',\n","    'Hi there, how can I help?'],\n","   'context_set': ''},\n","  {'tag': 'goodbye',\n","   'patterns': ['Bye', 'See you later', 'Goodbye'],\n","   'responses': ['See you later, thanks for visiting',\n","    'Have a nice day',\n","    'Bye! Come back again soon.']},\n","  {'tag': 'thanks',\n","   'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n","   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n","  {'tag': 'hours',\n","   'patterns': ['What hours are you open?',\n","    'What are your hours?',\n","    'When are you open?'],\n","   'responses': [\"We're open every day 9am-9pm\",\n","    'Our hours are 9am-9pm every day']},\n","  {'tag': 'location',\n","   'patterns': ['What is your location?',\n","    'Where are you located?',\n","    'What is your address?',\n","    'Where is your restaurant situated?'],\n","   'responses': ['We are on the intersection of London Alley and Bridge Avenue.',\n","    'We are situated at the intersection of London Alley and Bridge Avenue',\n","    'Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK']},\n","  {'tag': 'payments',\n","   'patterns': ['Do you take credit cards?',\n","    'Do you accept Mastercard?',\n","    'Are you cash only?'],\n","   'responses': ['We accept VISA, Mastercard and AMEX',\n","    'We accept most major credit cards']},\n","  {'tag': 'todaysmenu',\n","   'patterns': ['What is your menu for today?',\n","    'What are you serving today?',\n","    \"What is today's special?\"],\n","   'responses': [\"Today's special is Chicken Tikka\",\n","    'Our speciality for today is Chicken Tikka']},\n","  {'tag': 'deliveryoption',\n","   'patterns': ['Do you provide home delivery?',\n","    'Do you deliver the food?',\n","    'What are the home delivery options?'],\n","   'responses': ['Yes, we provide home delivery through UBER Eats and Zomato?',\n","    'We have home delivery options through UBER Eats and Zomato'],\n","   'context_set': 'food'},\n","  {'tag': 'menu',\n","   'patterns': ['What is your Menu?',\n","    'What are the main course options?',\n","    'Can you tell me the most delicious dish from the menu?',\n","    \"What is the today's special?\"],\n","   'responses': ['You can visit www.mymenu.com for menu options',\n","    'You can check out the food menu at www.mymenu.com',\n","    'You can check various delicacies given in the food menu at www.mymenu.com'],\n","   'context_filter': 'food'}]}"]},"metadata":{},"execution_count":3}],"source":["data"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KWeQ8ZAmf12w","executionInfo":{"status":"ok","timestamp":1749029179936,"user_tz":-330,"elapsed":40,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"outputs":[],"source":["import numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0oFoHlpkSMO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1749029181041,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"GLc7gc8RfKvF","outputId":"4374de58-6b37-4502-81ad-2b1a18985043"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('punkt_tab')\n","\n","words = []\n","classes = []\n","documents = []\n","ignore = ['?']\n","# loop through each sentence in the intent's patterns\n","for intent in data['intents']:\n","  for pattern in intent['patterns']:\n","    w = nltk.word_tokenize(pattern)\n","    # add word to the words list\n","    words.extend(w)\n","    # add word(s) to documents\n","    documents.append((w, intent['tag']))\n","    # add tags to our classes list\n","    if intent['tag'] not in classes:\n","      classes.append(intent['tag'])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3036,"status":"ok","timestamp":1749029186264,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"dPDvCR05yDPU","outputId":"bb660893-eec1-4818-dfef-c6fa9d549d04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tflearn in /usr/local/lib/python3.11/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.26.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.17.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tflearn) (11.2.1)\n","31 documents\n","9 classes ['deliveryoption', 'goodbye', 'greeting', 'hours', 'location', 'menu', 'payments', 'thanks', 'todaysmenu']\n","57 unique stemmed words [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["from IPython import get_ipython\n","from IPython.display import display\n","# %%\n","!pip install tflearn\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.stem.lancaster import LancasterStemmer\n","# Define and initialize the stemmer here\n","stemmer = LancasterStemmer()\n","\n","# Libraries needed for Tensorflow processing\n","import tensorflow as tf\n","import numpy as np\n"," # This should import successfully after installing with pip\n","import random\n","import json\n","# %%\n","import json\n","with open('intents.json') as f:\n","   data = json.load(f)\n","# %%\n","data\n","# %%\n","import numpy\n","# %%\n","\n","# %%\n","import nltk\n","# This download was moved from a later cell to ensure it's done early if needed\n","# although 'punkt' is downloaded in the first cell, having it here is fine.\n","nltk.download('punkt_tab')\n","\n","words = []\n","classes = []\n","documents = []\n","ignore = ['?']\n","# loop through each sentence in the intent's patterns\n","for intent in data['intents']:\n","  for pattern in intent['patterns']:\n","    w = nltk.word_tokenize(pattern)\n","    # add word to the words list\n","    words.extend(w)\n","    # add word(s) to documents\n","    documents.append((w, intent['tag']))\n","    # add tags to our classes list\n","    if intent['tag'] not in classes:\n","      classes.append(intent['tag'])\n","# %%\n","# Perform stemming and lower each word as well as remove duplicates\n","# stemmer is now defined because the first cell was executed\n","words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n","words = sorted(list(set(words)))\n","\n","# remove duplicate classes\n","classes = sorted(list(set(classes)))\n","\n","print (len(documents), \"documents\")\n","print (len(classes), \"classes\", classes)\n","print (len(words), \"unique stemmed words\", words)\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1749029188734,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"gfublVpJkS_j","outputId":"265c3ac0-ec54-455e-e840-3c4358c1c4e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train_x: (31, 57)\n","Shape of train_y: (31, 9)\n"]}],"source":["training = []\n","output = []\n","# create an empty array for output\n","output_empty = [0] * len(classes)\n","\n","# create training set, bag of words for each sentence\n","for doc in documents:\n","  bag = []\n","  # list of tokenized words for the pattern\n","  pattern_words = doc[0]\n","  # stemming each word\n","  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n","  # create bag of words array\n","  for w in words:\n","    bag.append(1) if w in pattern_words else bag.append(0)\n","  output_row = list(output_empty)\n","  output_row[classes.index(doc[1])] = 1\n","\n","  training.append([bag, output_row])\n","\n","                                                        # shuffling features and turning it into np.array\n","random.shuffle(training)\n","\n","train_x = np.array([item[0] for item in training]) # Extract bag of words for train_x\n","train_y = np.array([item[1] for item in training]) # Extract output_row for train_y\n","\n","# Print shapes for debugging\n","print(\"Shape of train_x:\", train_x.shape)\n","print(\"Shape of train_y:\", train_y.shape)"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S51yvBOqreM8","executionInfo":{"status":"ok","timestamp":1749032506354,"user_tz":-330,"elapsed":231,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}},"outputId":"d898f791-b3e1-4ca9-b9bf-3b302f98801c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: conda: command not found\n","/bin/bash: line 1: conda: command not found\n"]}],"source":["!conda create -n py37 python=3.7\n","!conda activate py37\n","\n"]},{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","!pip install tflearn\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xR6quqylcbwH","executionInfo":{"status":"ok","timestamp":1749032252564,"user_tz":-330,"elapsed":3887,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}},"outputId":"5acdcdea-ef35-4e14-d586-b18f8f70e304"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","2.18.0\n","Requirement already satisfied: tflearn in /usr/local/lib/python3.11/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.26.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tflearn) (1.17.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from tflearn) (11.2.1)\n"]}]},{"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import tflearn\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"w0b0LPQGam8M","executionInfo":{"status":"error","timestamp":1749031771242,"user_tz":-330,"elapsed":150,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}},"outputId":"69816644-54c1-4b7d-8b43-7f2f9130cd9a"},"execution_count":36,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-7b4263a83ed3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"oEQExTpCaoGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"elapsed":18,"status":"error","timestamp":1749031794131,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"},"user_tz":-330},"id":"j-Vf0S14p7Ln","outputId":"ea5264a6-bd2c-466e-ec85-da6102eea323"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-1023178f1bae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# If using TensorFlow 2.x or later:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import tflearn\n","\n","# If using TensorFlow 2.x or later:\n","tf.compat.v1.reset_default_graph()\n","\n","# Try importing is_sequence from tf.compat.v1.nest\n","try:\n","    from tensorflow.compat.v1.nest import is_sequence\n","    print(\"Successfully imported is_sequence from tf.compat.v1.nest\")\n","except ImportError:\n","    print(\"Could not import is_sequence from tf.compat.v1.nest. Consider downgrading TensorFlow.\")\n","    # You might need to explicitly add is_sequence to the tflearn.layers.recurrent module\n","    # if the above import works but tflearn still fails. This is a more advanced workaround.\n","    # For now, let's see if the import alone helps.\n","    pass # Allow tflearn import to potentially fail later if this doesn't fix it.\n","\n","\n","# Building neural network\n","net = tflearn.input_data(shape=[None, len(train_x[0])])\n","net = tflearn.fully_connected(net, 10)\n","net = tflearn.fully_connected(net, 10)\n","net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n","net = tflearn.regression(net)\n","\n","# Defining model and setting up tensorboard\n","model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n","\n","# Start training\n","# The traceback suggests the error happens during the tflearn import,\n","# so the fit and save calls are not reached yet.\n","# However, once the import is fixed, these should run.\n","try:\n","    model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n","    model.save('model.tflearn')\n","except Exception as e:\n","    print(f\"An error occurred during training or saving the model: {e}\")"]},{"cell_type":"code","source":["import pickle\n","pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"],"metadata":{"id":"t1o_XBNkSUiM","executionInfo":{"status":"ok","timestamp":1749029575221,"user_tz":-330,"elapsed":16,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","# restoring all the data structures\n","data = pickle.load( open( \"training_data\", \"rb\" ) )\n","words = data['words']\n","classes = data['classes']\n","train_x = data['train_x']\n","train_y = data['train_y']"],"metadata":{"id":"l5zBPz4vSaE0","executionInfo":{"status":"ok","timestamp":1749029597280,"user_tz":-330,"elapsed":47,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["with open('intents.json') as json_data:\n","    intents = json.load(json_data)"],"metadata":{"id":"Nc0N7HF6SeFJ","executionInfo":{"status":"ok","timestamp":1749029613617,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"uGU3fIVjSg-q","executionInfo":{"status":"error","timestamp":1749029918937,"user_tz":-330,"elapsed":154,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}},"outputId":"b790f9bd-6215-4c1e-af07-12b643d009c7"},"execution_count":23,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-16414abfc5a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensure you have imported tensorflow and tflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If using TensorFlow 2.x or later:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Predefined ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_normalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_response_normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional_rnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mBasicRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0membedding_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tflearn/layers/recurrent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/nest.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["def clean_up_sentence(sentence):\n","    # tokenizing the pattern\n","    sentence_words = nltk.word_tokenize(sentence)\n","    # stemming each word\n","    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","def bow(sentence, words, show_details=False):\n","    # tokenizing the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","    # generating bag of words\n","    bag = [0]*len(words)\n","    for s in sentence_words:\n","        for i,w in enumerate(words):\n","            if w == s:\n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","\n","    return(np.array(bag))"],"metadata":{"id":"YfQrWanoTDE8","executionInfo":{"status":"ok","timestamp":1749029765263,"user_tz":-330,"elapsed":3,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","ERROR_THRESHOLD = 0.30\n","def classify(sentence):\n","    # generate probabilities from the model\n","    results = model.predict([bow(sentence, words)])[0]\n","    # filter out predictions below a threshold\n","    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n","    # sort by strength of probability\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append((classes[r[0]], r[1]))\n","    # return tuple of intent and probability\n","    return return_list\n","\n","def response(sentence, userID='123', show_details=False):\n","    results = classify(sentence)\n","    # if we have a classification then find the matching intent tag\n","    if results:\n","        # loop as long as there are matches to process\n","        while results:\n","            for i in intents['intents']:\n","                # find a tag matching the first result\n","                if i['tag'] == results[0][0]:\n","                    # a random response from the intent\n","                    return print(random.choice(i['responses']))\n","\n","            results.pop(0)"],"metadata":{"id":"OeCcZtPsTJj9","executionInfo":{"status":"ok","timestamp":1749029791566,"user_tz":-330,"elapsed":77,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["classify('What are you hours of operation?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"H64h14GKTNoJ","executionInfo":{"status":"error","timestamp":1749029807774,"user_tz":-330,"elapsed":84,"user":{"displayName":"Anirban Dey","userId":"15725084108053054339"}},"outputId":"839d4574-2074-40d7-da72-aba37cf95438"},"execution_count":22,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-7d64f15674e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What are you hours of operation?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-e936f23f4f01>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# generate probabilities from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# filter out predictions below a threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mERROR_THRESHOLD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNeF7l63ktz+lgaGEdwSpOl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}